{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\.conda\\envs\\DAND\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import statsmodels.api as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 18, 6\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import operator \n",
    "import psycopg2\n",
    "import math\n",
    "import requests\n",
    "import json\n",
    "from flask import jsonify\n",
    "import csv\n",
    "from csv import writer\n",
    "from statistics import mean\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from numpy import cov\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'http://coronavirus-tracker-api.herokuapp.com/all'\n",
    "response = requests.get(url)\n",
    "lst = json.loads(response.text)\n",
    "url_iso2 = 'https://restcountries.eu/rest/v2/'\n",
    "\n",
    "\n",
    "response_iso2 = requests.get(url_iso2)\n",
    "lst_iso2 = json.loads(response_iso2.text)\n",
    "\n",
    "latestaffected={}\n",
    "latestdeath={}\n",
    "latestrecovered={}\n",
    "population={}\n",
    "dict_iso3 = {}\n",
    "\n",
    "\n",
    "for i in lst['confirmed']['locations']:\n",
    "    \n",
    "    latestaffected[i['country_code']] = {}\n",
    "    latestdeath[i['country_code']] = {}\n",
    "    latestrecovered[i['country_code']] = {}\n",
    "    \n",
    "for k in lst_iso2:\n",
    "    population[k['alpha2Code']] = k['population']\n",
    "    dict_iso3[k['alpha2Code']] = k['alpha3Code']\n",
    "    \n",
    "\n",
    "for i in lst['confirmed']['locations']:\n",
    "    code = i['country_code']\n",
    "    latestaffected[code] = 0\n",
    "\n",
    "for i in lst['recovered']['locations']:\n",
    "    code = i['country_code']\n",
    "    latestrecovered[code] = 0\n",
    "\n",
    "for i in lst['deaths']['locations']:\n",
    "    code = i['country_code']\n",
    "    latestdeath[code] = 0\n",
    "    \n",
    "\n",
    "    \n",
    "for i in lst['deaths']['locations']:\n",
    "    code = i['country_code']\n",
    "    latestdeath[code] = latestdeath[code] + i['latest'] \n",
    "\n",
    "for i in lst['recovered']['locations']:\n",
    "    code = i['country_code']\n",
    "    latestrecovered[code] = latestrecovered[code] + i['latest']\n",
    "\n",
    "for i in lst['confirmed']['locations']:\n",
    "    code = i['country_code']\n",
    "    latestaffected[code] = latestaffected[code] + i['latest']\n",
    "    \n",
    "for i in lst['confirmed']['locations']:\n",
    "    for j in i['history']:\n",
    "        latestdate = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7/8/20'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latestdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('countrylatestactive.csv', 'w') as csv_file:\n",
    "    csv_writer = writer(csv_file)\n",
    "    headers = ['Country_name', 'Country_code_iso2','population','Latest_date','Tot_affected','Tot_dead','Tot_recov','Affected per thousand','Active','Active_rateaff','Active_raterec']\n",
    "    csv_writer.writerow(headers)\n",
    "    for i in lst['confirmed']['locations']:\n",
    "        name = i['country']\n",
    "        code = i['country_code']\n",
    "        if code in dict_iso3:\n",
    "            active = latestaffected[code]-(latestrecovered[code]+ latestdeath[code])\n",
    "            entry = [name, code,population[code],latestdate,latestaffected[code],latestdeath[code],latestrecovered[code],(latestaffected[code]/population[code]) * 1000, active ,(active/latestaffected[code])*100,(active/(latestrecovered[code]+active))*100]\n",
    "            csv_writer.writerow(entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('countrylatestactive.csv','r') as in_file, open('countrylatestactive2.csv','w') as out_file:\n",
    "    seen = set() # set for fast O(1) amortized lookup\n",
    "    for line in in_file:\n",
    "        if line in seen: continue # skip duplicate\n",
    "\n",
    "        seen.add(line)\n",
    "        out_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching latest data of pollution and health from APi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://www.numbeo.com/api/rankings_by_country_historical?api_key=r8umol7u42vvbs&section=12&fbclid=IwAR2g23utsR5KC5PvEU91RyQk8Z3DWno1BMqEL-KsbgAG43aQc-RxX4TDN0w'\n",
    "response1 = requests.get(url1)\n",
    "lst1 = json.loads(response1.text)\n",
    "\n",
    "health_index = {}\n",
    "\n",
    "\n",
    "for i in lst1['2020']:\n",
    "    \n",
    "    health_index[i['iso3166_country_code']] = i['healthcare_index']\n",
    "    \n",
    "url2 = 'https://www.numbeo.com/api/rankings_by_country_historical?api_key=r8umol7u42vvbs&section=8&fbclid=IwAR2g23utsR5KC5PvEU91RyQk8Z3DWno1BMqEL-KsbgAG43aQc-RxX4TDN0w'\n",
    "response2 = requests.get(url2)\n",
    "lst2 = json.loads(response2.text)\n",
    "\n",
    "pollution_index = {}\n",
    "\n",
    "\n",
    "for i in lst2['2020']:\n",
    "    \n",
    "    pollution_index[i['iso3166_country_code']] = i['pollution_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('global_health_index_ranking_comma.csv', 'w') as csv_file:\n",
    "    csv_writer = writer(csv_file)\n",
    "    headers = ['country name', 'Country_code_iso2','healthcare index']\n",
    "    csv_writer.writerow(headers)\n",
    "    for i in lst1['2020']:\n",
    "        name = i['country']\n",
    "        code = i['iso3166_country_code']\n",
    "        entry = [name, code,health_index[code]]\n",
    "        csv_writer.writerow(entry)\n",
    "        \n",
    "with open('global_pollution_index_ranking_comma.csv', 'w') as csv_file:\n",
    "    csv_writer = writer(csv_file)\n",
    "    headers = ['country name', 'Country_code_iso2','pollution index']\n",
    "    csv_writer.writerow(headers)\n",
    "    for i in lst2['2020']:\n",
    "        name = i['country']\n",
    "        code = i['iso3166_country_code']\n",
    "        entry = [name, code,pollution_index[code]]\n",
    "        csv_writer.writerow(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making csv containg number of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "url3 = 'https://disease.sh/v2/countries'\n",
    "\n",
    "response3 = requests.get(url3)\n",
    "lst3 = json.loads(response3.text)\n",
    "\n",
    "dict_name = {}\n",
    "population = {}\n",
    "dict_iso3 = {}\n",
    "dict_noOfTests = {}\n",
    "total_cases = {}\n",
    "total_deaths = {}\n",
    "total_recovered = {}\n",
    "live_cases = {}\n",
    "live_deaths = {}\n",
    "live_recovered = {}\n",
    "active = {}\n",
    "critical = {}\n",
    "casesPerOneMillion = {}\n",
    "deathsPerOneMillion = {}\n",
    "testsPerOneMillion = {}\n",
    "recoveredPerOneMillion = {}\n",
    "criticalPerOneMillion = {}\n",
    "continent = {}\n",
    "\n",
    "\n",
    "with open('country_populationtests.csv','w',encoding=\"utf-8\") as csv_file:\n",
    "    csv_writer = writer(csv_file)\n",
    "    headers = ['Countryname', 'Countrycodeiso2', 'Countrycodeiso3','Continent','Population','Totaltests','Totalaffected','Totaldeaths','Totalrecovered','Active','Critical','Deathrate','Tests per million','Affected per million','Deaths per million','Recovered per million']\n",
    "    csv_writer.writerow(headers)\n",
    "    for i in lst3:\n",
    "        name = i['country']\n",
    "        iso2 = i['countryInfo']['iso2']\n",
    "        iso3 = i['countryInfo']['iso3']\n",
    "        noOfTests = i['tests']\n",
    "        population = i['population']\n",
    "        total_cases = i['cases']\n",
    "        total_deaths = i['deaths']\n",
    "        total_recovered =i['recovered']\n",
    "        active = i['active']\n",
    "        critical = i['critical']\n",
    "        casesPerOneMillion = i['casesPerOneMillion']\n",
    "        deathsPerOneMillion = i['deathsPerOneMillion']\n",
    "        testsPerOneMillion = i['testsPerOneMillion']\n",
    "        recoveredPerOneMillion = i['recoveredPerOneMillion']\n",
    "        criticalPerOneMillion = i['criticalPerOneMillion']\n",
    "        continent = i['continent']\n",
    "        \n",
    "        entry = [name, iso2, iso3,continent,population,noOfTests,total_cases,total_deaths,total_recovered,active,critical,(total_deaths/total_cases)*100,testsPerOneMillion,casesPerOneMillion,deathsPerOneMillion,recoveredPerOneMillion]\n",
    "        csv_writer.writerow(entry)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some data cleaning (0 deaths, <100 affected, 0 tests and tests< affected are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('country_populationtests.csv')\n",
    "finaldata = data[(data.Totaltests>0) & (data.Totalaffected>=100) & (data.Totaldeaths>0) & (data.Totaltests>=data.Totalaffected)]\n",
    "header = [\"Countryname\", \"Population\", \"Totaltests\", \"Deathrate\"]\n",
    "finaldata.to_csv('deathrate+tests.csv', columns = header,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining two csv files (final csv is combinedfood.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('global_food_index_ranking_comma.csv')\n",
    "df1 = pd.read_csv('countrylatestactive2.csv')\n",
    "res1 = pd.merge(df2, df1,on ='Country_code_iso2',how='left')\n",
    "res1.to_csv('combinedfood.csv', index=False)\n",
    "df3 = pd.read_csv('global_health_index_ranking_comma.csv')\n",
    "res2 = pd.merge(df3, df1,on ='Country_code_iso2',how='left')\n",
    "res2.to_csv('combinedhealth.csv', index=False)\n",
    "df4 = pd.read_csv('global_pollution_index_ranking_comma.csv')\n",
    "res3 = pd.merge(df4, df1,on ='Country_code_iso2',how='left')\n",
    "res3.to_csv('combinedpollution.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inserting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertInfo(date,method,independant,dependant,corelcoeff):\n",
    "    try:\n",
    "        postgres_insert_query = \"\"\"INSERT INTO crcdata(date,method,independant,dependant,corelcoeff)VALUES(%s,%s,%s,%s,%s)\"\"\"\n",
    "        cursor.execute(postgres_insert_query, (date,method,independant,dependant,corelcoeff))\n",
    "\n",
    "        connection.commit()\n",
    "        count = cursor.rowcount\n",
    "        print (count, \"Record inserted successfully into crcdata table\")\n",
    "        \n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        if(connection):\n",
    "            print(\"Error inserting data in PostgreSQL table\", error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def correl(bd,dependent,independent):\n",
    "    Bd1 = bd.iloc[:,2:3]\n",
    "    Bd2 = bd.iloc[:,3:4]\n",
    "    \n",
    "    \n",
    "    X = [x[0] for x in Bd1.values.tolist()]  #list comprehension\n",
    "    Y = [y[0] for y in Bd2.values.tolist()]\n",
    "    print('data1: mean=%.3f stdv=%.3f' % (mean(X), std(X)))\n",
    "    print('data2: mean=%.3f stdv=%.3f' % (mean(Y), std(Y)))\n",
    "    print('overall food security: median=%.3f' % statistics.median(X))\n",
    "    print('total affected: median=%.3f' % statistics.median(Y))\n",
    "    \n",
    "    \n",
    "    #print('my function pearson : %.5f' % pearson_def(X,Y))\n",
    "    \n",
    "    covariance = cov(X,Y)\n",
    "    print('covarience value :')\n",
    "    print( covariance)\n",
    "    \n",
    "#     fig = go.Figure()\n",
    "#     fig.add_trace(go.Scatter(x=X,y=Y,mode='markers',marker_color=Y)) # hover text goes here\n",
    "\n",
    "#     fig.update_layout(title='correlations', xaxis=dict(title='X'),yaxis=dict(title='Y'))\n",
    "    \n",
    "#     fig.show()\n",
    "    ##pyplot.scatter(Bd1, Bd2)\n",
    "    #pyplot.xlabel(\"temparature\")\n",
    "    #pyplot.ylabel(\"Total Affected\")\n",
    "    #pyplot.show()\n",
    "    alpha = 0.05 #setting my significant level 5 %\n",
    "    corr1, r = pearsonr(X,Y)\n",
    "    print('Pearsons correlation: %.3f' % corr1)\n",
    "   \n",
    "    insertInfo(latestdate,'Pearson',independent,dependent,corr1)\n",
    "    if r > alpha:\n",
    "        print('Samples are uncorrelated (fail to reject H0) p=%.3f' % r)\n",
    "        \n",
    "    else:\n",
    "        print('Samples are correlated (reject H0) p=%.3f' % r)\n",
    "        \n",
    "        \n",
    "    corr2, p = spearmanr(X,Y)\n",
    "    print('Spearmans correlation: %.3f' % corr2)\n",
    "    \n",
    "    insertInfo(latestdate,'Spearman',independent,dependent,corr2)\n",
    "    # interpret the significance\n",
    "    \n",
    "    if p > alpha:\n",
    "        print('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "        \n",
    "    else:\n",
    "        print('Samples are correlated (reject H0) p=%.3f' % p)\n",
    "       \n",
    "        \n",
    "    \n",
    "    coef, q = kendalltau(X,Y)\n",
    "    print('Kendall correlation coefficient: %.3f' % coef)\n",
    "    insertInfo(latestdate,'Kendall',independent,dependent,coef)\n",
    "    \n",
    "    # interpret the significance\n",
    "    alpha = 0.05\n",
    "    if q > alpha:\n",
    "        print('Samples are uncorrelated (fail to reject H0) p=%.3f' % q)\n",
    "        \n",
    "    else:\n",
    "        print('Samples are correlated (reject H0) p=%.3f' % q)\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('combinedfood.csv','r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    with open('affected+food.csv','w') as new_file1:\n",
    "        fieldnames1 = ['Country_name','population','Overall','Tot_affected','Latest_date']\n",
    "        \n",
    "        csv_writer1 = csv.DictWriter(new_file1, fieldnames= fieldnames1)\n",
    "        csv_writer1.writeheader()\n",
    "        \n",
    "        for line1 in csv_reader:\n",
    "            del line1['Region']\n",
    "            del line1['idcount']\n",
    "            del line1['Country_code_iso2']\n",
    "            del line1['Country3']\n",
    "            del line1['Date']\n",
    "            del line1['Affordability']\n",
    "            del line1['Availability']\n",
    "            del line1['Safety']\n",
    "            del line1['Tot_dead']\n",
    "            del line1['Tot_recov']\n",
    "            del line1['Affected per thousand']\n",
    "            del line1['Active']\n",
    "            del line1['Active_rateaff']\n",
    "            del line1['Active_raterec']\n",
    "            #print(line['CFR1'],line['CFR2'])\n",
    "            #print(line1)\n",
    "            csv_writer1.writerow(line1)\n",
    "            \n",
    "with open('combinedhealth.csv','r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    with open('recovered+health.csv','w') as new_file1:\n",
    "        fieldnames1 = ['country name','population','healthcare index','Tot_recov','Latest_date']\n",
    "        \n",
    "        csv_writer1 = csv.DictWriter(new_file1, fieldnames= fieldnames1)\n",
    "        csv_writer1.writeheader()\n",
    "        \n",
    "        for line1 in csv_reader:\n",
    "            del line1['Country_code_iso2']\n",
    "            del line1['Tot_affected']\n",
    "            del line1['Tot_dead']\n",
    "            del line1['Affected per thousand']\n",
    "            del line1['Active']\n",
    "            del line1['Active_rateaff']\n",
    "            del line1['Active_raterec']\n",
    "            del line1['Country_name']\n",
    "            #print(line['CFR1'],line['CFR2'])\n",
    "            #print(line1)\n",
    "            csv_writer1.writerow(line1)\n",
    "            \n",
    "with open('combinedpollution.csv','r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    with open('affected+pollution.csv','w') as new_file1:\n",
    "        fieldnames1 = ['country name','population','pollution index','Tot_affected','Latest_date']\n",
    "        \n",
    "        csv_writer1 = csv.DictWriter(new_file1, fieldnames= fieldnames1)\n",
    "        csv_writer1.writeheader()\n",
    "        \n",
    "        for line1 in csv_reader:\n",
    "            del line1['Country_code_iso2']\n",
    "            del line1['Tot_recov']\n",
    "            del line1['Tot_dead']\n",
    "            del line1['Affected per thousand']\n",
    "            del line1['Active']\n",
    "            del line1['Active_rateaff']\n",
    "            del line1['Active_raterec']\n",
    "            del line1['Country_name']\n",
    "            #print(line['CFR1'],line['CFR2'])\n",
    "            #print(line1)\n",
    "            csv_writer1.writerow(line1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1: mean=62.888 stdv=13.819\n",
      "data2: mean=102097.230 stdv=340108.455\n",
      "overall food security: median=63.800\n",
      "total affected: median=14916.000\n",
      "covarience value :\n",
      "[[1.92676452e+02 1.05837001e+06]\n",
      " [1.05837001e+06 1.16706562e+11]]\n",
      "Pearsons correlation: 0.223\n",
      "Error inserting data in PostgreSQL table duplicate key value violates unique constraint \"crcdata_pkey\"\n",
      "DETAIL:  Key (date, method, independant, dependant)=(2020-07-08, Pearson, Food Security Index, Total Affected) already exists.\n",
      "\n",
      "Samples are correlated (reject H0) p=0.017\n",
      "Spearmans correlation: 0.565\n",
      "Error inserting data in PostgreSQL table current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "Samples are correlated (reject H0) p=0.000\n",
      "Kendall correlation coefficient: 0.385\n",
      "Error inserting data in PostgreSQL table current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "Samples are correlated (reject H0) p=0.000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cuntry name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\DAND\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cuntry name'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-32b5c081a185>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mcorrel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfood\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Total Affected'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Food Security Index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mhealth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'recovered+health.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mhealth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhealth\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhealth\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cuntry name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m'Hong Kong'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mcorrel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhealth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Total Recovered'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Heath Index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DAND\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DAND\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cuntry name'"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "connection = psycopg2.connect(user = \"teamcoronosis\",\n",
    "                                  password = \"p1jGQwEMANAGZ5b72xWh\",\n",
    "                                  host = \"coronosis.cfzyz28p01p7.ap-south-1.rds.amazonaws.com\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"postgres\")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "food = pd.read_csv('affected+food.csv')\n",
    "\n",
    "correl(food,'Total Affected','Food Security Index')\n",
    "health = pd.read_csv('recovered+health.csv')\n",
    "health = health[health['country name']!='Hong Kong']\n",
    "correl(health,'Total Recovered','Heath Index')\n",
    "\n",
    "pollution = pd.read_csv('affected+pollution.csv')\n",
    "pollution = pollution[(pollution['country name']!='Puerto Rico') & (pollution['country name']!='Hong Kong')]\n",
    "correl(pollution,'Total Affected','Pollution Index')\n",
    "tests = pd.read_csv('deathrate+tests.csv')\n",
    "\n",
    "correl(tests,'Death Rate','Total Tests')\n",
    "insertInfo(latestdate,'Pearson','Temperature','Affected',0.0866)\n",
    "insertInfo(latestdate,'Spearson','Temperature','Affected',0.0930)\n",
    "insertInfo(latestdate,'Kendall','Temperature','Affected',0.0777)\n",
    "insertInfo(latestdate,'Pearson','Humidity','Affected',-0.4951)\n",
    "insertInfo(latestdate,'Spearson','Humidity','Affected',-0.4366)\n",
    "insertInfo(latestdate,'Kendall','Humidity','Affected',-0.3155)\n",
    "\n",
    "\n",
    "if(connection):\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"PostgreSQL connection is closed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-DAND] *",
   "language": "python",
   "name": "conda-env-.conda-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
